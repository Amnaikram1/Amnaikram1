{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNn7fJe2O0wUtUi2lVDXUUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amnaikram1/Amnaikram1/blob/master/notebooks\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "id": "CuP6qJzsckNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ74pbLdVfHk"
      },
      "outputs": [],
      "source": [
        "import openai  # For interacting with the OpenAI API\n",
        "import time  # For adding delays between API calls\n",
        "import pandas as pd  # For handling data in DataFrame format\n",
        "import numpy as np  # For numerical operations\n",
        "import os  # For interacting with the operating system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreProcessor:\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Initialize the DataPreProcessor with a DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame containing the data to analyze.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"\n",
        "        Clean the data by filling missing values and extracting the day name from the date.\n",
        "        \"\"\"\n",
        "        self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "        self.df['Day'] = self.df['Date'].dt.day_name()\n",
        "\n",
        "        mode = self.df['Tag'].mode().iloc[0]\n",
        "        self.df['Tag'] = self.df['Tag'].fillna(mode)\n",
        "\n",
        "        mean_hours = self.df['Hours'].mean()\n",
        "        self.df['Hours'] = self.df['Hours'].fillna(mean_hours)\n",
        "\n",
        "        self.df['Task Description'] = self.df['Task Description'].fillna('No Task')\n",
        "\n",
        "        self.df['Billable'] = self.df['Billable'].fillna('False')\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def mapping_of_tags(self):\n",
        "        \"\"\"\n",
        "        Map tags to predefined categories and update the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        dict: A dictionary mapping each tag to its corresponding category.\n",
        "        \"\"\"\n",
        "        # Define the mapping of tags to categories\n",
        "        category_mapping = {\n",
        "            'Meeting': ['Meeting', 'Standup', 'Team Discussion', 'Team Engagement', 'Coordination',\n",
        "                        'Team Management', 'Brainstorming', 'Check-in'],\n",
        "            'Learning': ['Training/Learning', 'Mentoring/Coaching'],\n",
        "            'Code': ['Coding', 'Code Review', 'Debugging']\n",
        "        }\n",
        "\n",
        "        # Reverse the mapping to create a tag-to-category dictionary\n",
        "        tag_to_category = {task: category for category, tasks in category_mapping.items() for task in tasks}\n",
        "\n",
        "        # Map tags in the DataFrame to their corresponding categories\n",
        "        self.df['Tag'] = self.df['Tag'].apply(lambda x: tag_to_category.get(x, x))\n",
        "\n",
        "        return tag_to_category\n",
        "\n",
        "class AnomalyDetectorWorkingHours:\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalyDetectorWorkingHours with a DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame containing the data to analyze.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.grouped_df = None\n",
        "        self.anomalies = None\n",
        "\n",
        "    def group_data(self):\n",
        "        \"\"\"\n",
        "        Group the data by 'Sub-team', 'Tag', 'Day', 'Date', and 'Person Name',\n",
        "        and calculate the sum of 'Hours'.\n",
        "        \"\"\"\n",
        "        self.grouped_df = self.df.groupby(['Sub-team', 'Tag', 'Day', 'Date', 'Person Name'])['Hours'].sum().reset_index()\n",
        "\n",
        "    def add_avg_hours_info(self):\n",
        "        \"\"\"\n",
        "        Add average hours information to the grouped DataFrame.\n",
        "\n",
        "        This includes:\n",
        "        - Avg_Hours_Per_Sub-team: The average hours per sub-team.\n",
        "        - Avg_Hours_Across_All_Teamss: The average hours per tag across all teams.\n",
        "        \"\"\"\n",
        "        self.grouped_df['Avg_Hours_Per_Sub-team'] = self.grouped_df.groupby(['Sub-team', 'Tag', 'Day'])['Hours'].transform(lambda x: round(x.mean(), 2))\n",
        "        self.grouped_df['Avg_Hours_Across_All_Teams'] = self.grouped_df.groupby('Tag')['Hours'].transform(lambda x: round(x.mean(), 2))\n",
        "\n",
        "    def modified_z_score(self, data):\n",
        "        \"\"\"\n",
        "        Calculate the modified Z-score of a data series.\n",
        "\n",
        "        Parameters:\n",
        "        data (pd.Series): The input data series.\n",
        "\n",
        "        Returns:\n",
        "        pd.Series: The modified Z-scores of the input data.\n",
        "        \"\"\"\n",
        "        median = np.median(data)\n",
        "        mad = np.median(np.abs(data - median))\n",
        "        modified_z_scores = 0.6745 * (data - median) / mad if mad else np.zeros(len(data))\n",
        "        return modified_z_scores\n",
        "\n",
        "    def calculate_modified_z_scores(self):\n",
        "        \"\"\"\n",
        "        Calculate the modified Z-scores for 'Hours' grouped by 'Sub-team', 'Tag', and 'Date'.\n",
        "        \"\"\"\n",
        "        self.grouped_df['Z_Score'] = self.grouped_df.groupby(['Sub-team', 'Tag', 'Date'])['Hours'].transform(self.modified_z_score)\n",
        "\n",
        "    def identify_anomalies(self):\n",
        "        \"\"\"\n",
        "        Identify anomalies based on modified Z-scores greater than 3.5.\n",
        "        \"\"\"\n",
        "        self.anomalies = self.grouped_df[(self.grouped_df['Z_Score'] > 0) & (self.grouped_df['Z_Score'] > 3.5)].copy()\n",
        "        self.anomalies['Anomaly_Type'] = 'Hours Anomaly'\n",
        "\n",
        "    def print_anomalies(self):\n",
        "        \"\"\"\n",
        "        Print the anomalies identified.\n",
        "        \"\"\"\n",
        "        if self.anomalies is not None and not self.anomalies.empty:\n",
        "            print(self.anomalies[['Sub-team', 'Tag', 'Day', 'Date', 'Person Name', 'Hours', 'Z_Score', 'Avg_Hours_Per_Sub-team', 'Avg_Hours_Across_All_Teams', 'Anomaly_Type']])\n",
        "        else:\n",
        "            print(\"No anomalies found or anomalies have not been calculated yet.\")\n",
        "\n",
        "# Usage\n",
        "df_1 = pd.read_csv('/content/drive/MyDrive/Insurify.csv')\n",
        "\n",
        "# Step 1: Preprocess the data using DataPreProcessor\n",
        "preprocessor = DataPreProcessor(df_1)\n",
        "df_cleaned = preprocessor.clean_data()\n",
        "preprocessor.mapping_of_tags()\n",
        "\n",
        "# Step 2: Initialize and use the anomaly detector\n",
        "detector = AnomalyDetectorWorkingHours(df_cleaned)\n",
        "detector.group_data()\n",
        "detector.add_avg_hours_info()\n",
        "detector.calculate_modified_z_scores()\n",
        "detector.identify_anomalies()\n",
        "detector.print_anomalies()\n",
        "\n",
        "# Step 3: Save the anomalies to a CSV file\n",
        "anomalies_df = detector.anomalies\n",
        "anomalies_df.to_csv('working_hours_anomalies.csv', index=False)\n"
      ],
      "metadata": {
        "id": "1HKpfVoocnjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreProcessor:\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Initialize the DataPreProcessor with a DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame containing the data to analyze.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"\n",
        "        Clean the data by filling missing values and extracting the day name from the date.\n",
        "        \"\"\"\n",
        "        self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "        self.df['Day'] = self.df['Date'].dt.day_name()\n",
        "\n",
        "        mode = self.df['Tag'].mode().iloc[0]\n",
        "        self.df['Tag'] = self.df['Tag'].fillna(mode)\n",
        "\n",
        "        mean_hours = self.df['Hours'].mean()\n",
        "        self.df['Hours'] = self.df['Hours'].fillna(mean_hours)\n",
        "\n",
        "        self.df['Task Description'] = self.df['Task Description'].fillna('No Task')\n",
        "\n",
        "        self.df['Billable'] = self.df['Billable'].fillna('False')\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def mapping_of_tags(self):\n",
        "        \"\"\"\n",
        "        Map tags to predefined categories and update the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        dict: A dictionary mapping each tag to its corresponding category.\n",
        "        \"\"\"\n",
        "        # Define the mapping of tags to categories\n",
        "        category_mapping = {\n",
        "            'Meeting': ['Meeting', 'Standup', 'Team Discussion', 'Team Engagement', 'Coordination',\n",
        "                        'Team Management', 'Brainstorming', 'Check-in'],\n",
        "            'Learning': ['Training/Learning', 'Mentoring/Coaching'],\n",
        "            'Code': ['Coding', 'Code Review', 'Debugging']\n",
        "        }\n",
        "\n",
        "        # Reverse the mapping to create a tag-to-category dictionary\n",
        "        tag_to_category = {task: category for category, tasks in category_mapping.items() for task in tasks}\n",
        "\n",
        "        # Map tags in the DataFrame to their corresponding categories\n",
        "        self.df['Tag'] = self.df['Tag'].apply(lambda x: tag_to_category.get(x, x))\n",
        "\n",
        "        return tag_to_category\n",
        "\n",
        "class AnomalyDetectorBillingHours:\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalyDetectorBillingHours with a DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame containing the data to analyze.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.grouped_df = None\n",
        "        self.anomalies = None\n",
        "\n",
        "    def group_billable_data(self):\n",
        "        \"\"\"\n",
        "        Group the billable data by 'Sub-team', 'Tag', 'Day', 'Date', and 'Person Name',\n",
        "        and calculate the sum of 'Hours'.\n",
        "        \"\"\"\n",
        "        # Ensure Billable is treated as boolean\n",
        "        self.df['Billable'] = self.df['Billable'].astype(str).str.strip().str.lower() == 'true'\n",
        "        billable_df = self.df[self.df['Billable']]\n",
        "        self.grouped_df = billable_df.groupby(['Sub-team', 'Tag', 'Day', 'Date', 'Person Name'])['Hours'].sum().reset_index()\n",
        "\n",
        "    def add_avg_hours_info(self):\n",
        "        \"\"\"\n",
        "        Add average hours information to the grouped DataFrame.\n",
        "\n",
        "        This includes:\n",
        "        - Avg_Hours_Per_Sub-team: The average hours per tag per day per subteam.\n",
        "        - Avg_Hours_Across_All_Teams: The average hours per tag across all teams.\n",
        "        \"\"\"\n",
        "        self.grouped_df['Avg_Hours_Per_Sub-team'] = self.grouped_df.groupby(['Sub-team', 'Tag', 'Day'])['Hours'].transform(lambda x: round(x.mean(), 2))\n",
        "        self.grouped_df['Avg_Hours_Across_All_Teams'] = self.grouped_df.groupby('Tag')['Hours'].transform(lambda x: round(x.mean(), 2))\n",
        "\n",
        "    def z_score(self, data):\n",
        "        \"\"\"\n",
        "        Calculate the z-score of a data series.\n",
        "\n",
        "        Parameters:\n",
        "        data (pd.Series): The input data series.\n",
        "\n",
        "        Returns:\n",
        "        pd.Series: The z-scores of the input data.\n",
        "        \"\"\"\n",
        "        mean = np.mean(data)\n",
        "        std_dev = np.std(data)\n",
        "        return (data - mean) / std_dev if std_dev != 0 else np.zeros(len(data))\n",
        "\n",
        "    def calculate_z_scores(self):\n",
        "        \"\"\"\n",
        "        Calculate the z-scores for 'Hours' grouped by 'Sub-team', 'Tag', and 'Date'.\n",
        "        \"\"\"\n",
        "        self.grouped_df['Z_Score'] = self.grouped_df.groupby(['Sub-team', 'Tag', 'Date'])['Hours'].transform(self.z_score)\n",
        "\n",
        "    def identify_anomalies(self):\n",
        "        \"\"\"\n",
        "        Identify anomalies based on z-scores greater than 2 and add an 'Anomaly_Type' column.\n",
        "        \"\"\"\n",
        "        self.anomalies = self.grouped_df[(self.grouped_df['Z_Score'] > 0) & (self.grouped_df['Z_Score'] > 2)].copy()\n",
        "        self.anomalies['Anomaly_Type'] = 'Billing Anomaly'\n",
        "\n",
        "    def print_anomalies(self):\n",
        "        \"\"\"\n",
        "        Print the anomalies identified.\n",
        "        \"\"\"\n",
        "        if self.anomalies is not None and not self.anomalies.empty:\n",
        "            print(self.anomalies[['Sub-team', 'Tag', 'Day', 'Date', 'Person Name', 'Hours', 'Z_Score', 'Avg_Hours_Per_Sub-team', 'Avg_Hours_Across_All_Teams', 'Anomaly_Type']])\n",
        "        else:\n",
        "            print(\"No anomalies found or anomalies have not been calculated yet.\")\n",
        "\n",
        "# Usage\n",
        "df_1 = pd.read_csv('/content/drive/MyDrive/Insurify.csv')\n",
        "\n",
        "# Step 1: Preprocess the data using DataPreProcessor\n",
        "preprocessor = DataPreProcessor(df_1)\n",
        "df_cleaned = preprocessor.clean_data()\n",
        "preprocessor.mapping_of_tags()\n",
        "\n",
        "# Step 2: Initialize and use the anomaly detector\n",
        "detector = AnomalyDetectorBillingHours(df_cleaned)\n",
        "detector.group_billable_data()\n",
        "detector.add_avg_hours_info()\n",
        "detector.calculate_z_scores()\n",
        "detector.identify_anomalies()\n",
        "detector.print_anomalies()\n",
        "\n",
        "# Save anomalies to CSV\n",
        "anomalies_df = detector.anomalies\n",
        "anomalies_df.to_csv('billing_hours_anomalies.csv', index=False)\n"
      ],
      "metadata": {
        "id": "8fU5QcGYcxNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalySummaryGenerator:\n",
        "    def __init__(self, file_paths):\n",
        "        \"\"\"\n",
        "        Initialize the AnomalySummaryGenerator with CSV file paths.\n",
        "\n",
        "        Parameters:\n",
        "        file_paths (list): A list containing paths to CSV files with anomalous data.\n",
        "        \"\"\"\n",
        "        self.df = self.load_and_concat_data(file_paths)\n",
        "        self.api_key = os.getenv('OPENAI_API_KEY')\n",
        "        openai.api_key = self.api_key\n",
        "\n",
        "    def load_and_concat_data(self, file_paths):\n",
        "        \"\"\"\n",
        "        Load and concatenate CSV files into a single DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        file_paths (list): A list containing paths to CSV files.\n",
        "\n",
        "        Returns:\n",
        "        pd.DataFrame: Concatenated DataFrame of the loaded CSV files.\n",
        "        \"\"\"\n",
        "        dataframes = [pd.read_csv(file_path) for file_path in file_paths]\n",
        "        return pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    def generate_summary(self, row):\n",
        "        \"\"\"\n",
        "        Generates a summary report from an anomalous row of the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        row (pd.Series): A single row of the DataFrame containing anomalous data.\n",
        "\n",
        "        Returns:\n",
        "        str: The generated summary report.\n",
        "        \"\"\"\n",
        "       prompt = (\n",
        "            f\"Generate a one-line summary for the following anomalous ERP log entry from a company. \"\n",
        "            f\"Describe the anomaly clearly and professionally, using the appropriate terminology based on the type of anomaly and highlighting any negative behavior:\"\n",
        "            f\"- For 'Billing Anomaly': Emphasize the individual's significantly higher billing hours compared to the average, suggesting unusual billing activity for that category.\"\n",
        "            f\"- For 'Hours Anomaly': Summarize conversationally, noting if an individual spent substantially more hours on a category than the average, especially when their peers did not exceed the average hours in that category.\"\n",
        "            f\"Include the person's name, date, day of the week, anomaly type, category, total hours spent, and average hours spent by other team members.\"\n",
        "            f\"Details: Person Name: {row['Person Name']}, Date: {row['Date']}, Day: {pd.to_datetime(row['Date']).day_name()}, Anomaly Type: {row['Anomaly_Type']}, \"\n",
        "            f\"Category: {row['Tag']}, Hours Spent: {row['Hours']}, Average Hours: {row['Avg_Hours_Per_Sub-team']}, Sub-team: {row['Sub-team']}\"\n",
        "        )\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=1500,\n",
        "            temperature=0.0,\n",
        "            top_p=1,\n",
        "            frequency_penalty=1,\n",
        "            presence_penalty=0\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message['content'].strip()\n",
        "\n",
        "    def generate_summaries(self):\n",
        "        \"\"\"\n",
        "        Generate summaries for all rows in the DataFrame and save the results to a CSV file.\n",
        "\n",
        "        Returns:\n",
        "        pd.DataFrame: DataFrame containing the generated summaries.\n",
        "        \"\"\"\n",
        "        summaries_list = []\n",
        "\n",
        "        for index, row in self.df.iterrows():\n",
        "            # Create the summary\n",
        "            summary = self.generate_summary(row)\n",
        "\n",
        "            time.sleep(5)  # To prevent rate limits\n",
        "\n",
        "            # Append the summary to the summaries list\n",
        "            summaries_list.append({\n",
        "                'Name': row['Person Name'],\n",
        "                'Date': row['Date'],\n",
        "                'Sub-Team': row['Sub-team'],\n",
        "                'Tag': row['Tag'],\n",
        "                'Hours': row['Hours'],\n",
        "                'Summary': summary\n",
        "            })\n",
        "\n",
        "        summaries_df = pd.DataFrame(summaries_list)\n",
        "\n",
        "        # Save the full summaries DataFrame to CSV\n",
        "        summaries_df.to_csv('summaries_with_anomalies.csv', index=False)\n",
        "\n",
        "        return summaries_df\n",
        "\n",
        "# Usage\n",
        "file_paths = ['/content/working_hours_anomalies.csv', '/content/billing_hours_anomalies.csv']\n",
        "summary_generator = AnomalySummaryGenerator(file_paths)\n",
        "summaries_df = summary_generator.generate_summaries()\n",
        "\n"
      ],
      "metadata": {
        "id": "XiXyE5S0c0Kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}